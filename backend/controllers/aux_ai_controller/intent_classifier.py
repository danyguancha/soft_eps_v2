# controllers/aux_ai_controller/intent_classifier.py
import spacy
from spacy.matcher import Matcher
from typing import Dict, Tuple
import numpy as np
import re


# Constantes para literales duplicados - Intenciones
INTENT_STATISTICAL = 'statistical'
INTENT_STRUCTURE = 'structure_analysis'
INTENT_GREETING = 'greeting'
INTENT_GENERAL = 'general'

# Constantes para variaciones de "estad칤stica" (elimina duplicaci칩n)
LITERAL_ESTADISTICAS = 'estad칤sticas'
LITERAL_ESTADISTICA = 'estad칤stica'
LITERAL_ESTADISTICO = 'estadistico'
LITERAL_ESTADISTICOS = 'estad칤sticos'

# Lista de variaciones para uso en patrones
ESTADISTICA_VARIANTS = [
    LITERAL_ESTADISTICAS,
    LITERAL_ESTADISTICA,
    LITERAL_ESTADISTICO,
    LITERAL_ESTADISTICOS
]

# Palabras clave de estad칤sticas
STATS_KEYWORDS = [
    LITERAL_ESTADISTICAS, LITERAL_ESTADISTICA, LITERAL_ESTADISTICO, LITERAL_ESTADISTICOS,
    'promedio', 'media', 'mediana', 'moda',
    'suma', 'total', 'conteo',
    'm치ximo', 'm칤nimo', 'rango',
    'desviaci칩n', 'varianza', 'frecuencia',
    'distribuci칩n', 'percentil'
]

STRUCTURE_KEYWORDS = ['columnas', 'campos', 'estructura']


class IntentClassifier:
    """Clasificador mejorado con detecci칩n robusta de estad칤sticas"""
    
    def __init__(self):
        try:
            self.nlp = spacy.load("es_core_news_lg")
            print("Modelo spaCy large cargado")
        except OSError:
            import subprocess
            subprocess.run(["python", "-m", "spacy", "download", "es_core_news_lg"])
            self.nlp = spacy.load("es_core_news_lg")
        
        self.intent_examples = {
            INTENT_GREETING: [
                "hola", "buenos d칤as", "buenas tardes", "hey", "qu칠 tal"
            ],
            INTENT_STRUCTURE: [
                "qu칠 columnas tiene", "muestra las columnas", 
                "cu치les son los campos", "estructura del archivo",
                "columnas disponibles", "listar columnas"
            ],
            INTENT_STATISTICAL: [
                f"realiza {LITERAL_ESTADISTICAS}", f"genera {LITERAL_ESTADISTICAS}", 
                f"generame {LITERAL_ESTADISTICAS}",
                f"calcula {LITERAL_ESTADISTICAS}", f"dame {LITERAL_ESTADISTICAS}", 
                f"{LITERAL_ESTADISTICAS} del archivo",
                f"hacer {LITERAL_ESTADISTICAS}", f"obtener {LITERAL_ESTADISTICAS}", 
                f"mostrar {LITERAL_ESTADISTICAS}",
                f"an치lisis {LITERAL_ESTADISTICO}", f"resumen {LITERAL_ESTADISTICO}", 
                f"{LITERAL_ESTADISTICAS} descriptivas",
                "cu치l es el promedio", "calcula la media", "dame la mediana",
                "suma total", "contar registros", "m치ximo y m칤nimo",
                "desviaci칩n est치ndar", "frecuencias", "distribuci칩n",
                "m칠tricas", "an치lisis num칠rico", "valores estad칤sticos"
            ],
            'filtering': [
                "filtrar por", "buscar donde", "encontrar registros"
            ],
            'temporal': [
                "an치lisis temporal", "tendencia", "serie de tiempo"
            ],
            'help': [
                "c칩mo puedo", "ay칰dame", "explica c칩mo"
            ],
            'export': [
                "exportar datos", "descargar archivo"
            ],
            'comparison': [
                "comparar", "diferencia entre"
            ]
        }
        
        self.intent_vectors = self._create_intent_vectors()
        self._setup_patterns()
    
    def _create_intent_vectors(self) -> Dict[str, np.ndarray]:
        """Crea vectores para cada intenci칩n"""
        intent_vectors = {}
        
        for intent, examples in self.intent_examples.items():
            vectors = []
            for example in examples:
                doc = self.nlp(example)
                if doc.has_vector:
                    vectors.append(doc.vector)
            
            if vectors:
                intent_vectors[intent] = np.mean(vectors, axis=0)
        
        return intent_vectors
    
    def _setup_patterns(self):
        """Configura patrones mejorados"""
        self.matcher = Matcher(self.nlp.vocab)
        
        # PATR칍N 1: [VERBO] + estad칤sticas
        pattern_stats_verb = [
            {"LEMMA": {"IN": [
                "realizar", "generar", "calcular", "obtener", 
                "hacer", "dame", "mostrar", "crear", "producir"
            ]}},
            {"OP": "*", "IS_PUNCT": False},
            {"LOWER": {"IN": ESTADISTICA_VARIANTS}}  # Usando constante
        ]
        self.matcher.add("STATS_VERB", [pattern_stats_verb])
        
        # PATR칍N 2: estad칤sticas + de/del + [archivo/datos]
        pattern_stats_of = [
            {"LOWER": {"IN": [LITERAL_ESTADISTICAS, LITERAL_ESTADISTICA]}},  # Usando constantes
            {"LOWER": {"IN": ["de", "del", "para"]}},
            {"OP": "*"}
        ]
        self.matcher.add("STATS_OF", [pattern_stats_of])
        
        # PATR칍N 3: Solo "estad칤sticas" en consulta corta
        pattern_stats_simple = [
            {"LOWER": {"IN": [LITERAL_ESTADISTICAS, LITERAL_ESTADISTICA, LITERAL_ESTADISTICOS]}}  # Usando constantes
        ]
        self.matcher.add("STATS_SIMPLE", [pattern_stats_simple])
        
        # Patr칩n para estructura
        pattern_structure = [
            {"LEMMA": {"IN": ["mostrar", "listar", "ver", "ense침ar"]}},
            {"OP": "*"},
            {"LOWER": {"IN": STRUCTURE_KEYWORDS}}
        ]
        self.matcher.add("STRUCTURE_PATTERN", [pattern_structure])
    
    def _check_regex_patterns(self, text_lower: str) -> Tuple[bool, float]:
        """Verifica patrones regex para estad칤sticas"""
        stats_patterns = [
            r'\b(genera|realiza|calcula|dame|obtener|hacer|muestra|crea)\s+(?:las?\s+)?estad[i칤]sticas?\b',
            r'\bestad[i칤]sticas?\s+(?:de|del|para)\b',
            r'\bestad[i칤]sticas?\s+(?:descriptivas?|generales?)\b',
            r'\bestad[i칤]sticas?\b.*\barchivo\b',
            r'\barchivo\b.*\bestad[i칤]sticas?\b'
        ]
        
        for pattern in stats_patterns:
            if re.search(pattern, text_lower, re.IGNORECASE):
                print(f"{LITERAL_ESTADISTICAS.upper()} detectadas por regex")  # Usando constante
                return (True, 0.98)
        
        return (False, 0.0)
    
    def _check_syntactic_patterns(self, doc, text_lower: str) -> Tuple[str, float]:
        """Verifica patrones sint치cticos con spaCy"""
        matches = self.matcher(doc)
        if not matches:
            return (None, 0.0)
        
        match_names = [self.nlp.vocab.strings[match_id] for match_id, start, end in matches]
        
        # Prioridad: patrones de estad칤sticas
        if any(name in match_names for name in ["STATS_VERB", "STATS_OF", "STATS_SIMPLE"]):
            print(f"{LITERAL_ESTADISTICAS.upper()} detectadas por patr칩n sint치ctico")  # Usando constante
            return (INTENT_STATISTICAL, 0.97)
        
        # Solo estructura si menciona columnas Y NO estad칤sticas
        if "STRUCTURE_PATTERN" in match_names and "estad" not in text_lower:
            print("游댌 ESTRUCTURA detectada")
            return (INTENT_STRUCTURE, 0.95)
        
        return (None, 0.0)
    
    def _check_keyword_match(self, text_lower: str) -> Tuple[str, float]:
        """Verifica coincidencias de palabras clave"""
        stats_count = sum(1 for keyword in STATS_KEYWORDS if keyword in text_lower)
        
        if stats_count >= 1:
            structure_count = sum(1 for keyword in STRUCTURE_KEYWORDS if keyword in text_lower)
            
            if structure_count == 0 or stats_count > structure_count:
                print(f"{LITERAL_ESTADISTICAS.upper()} detectadas ({stats_count} palabras clave)")  # Usando constante
                return (INTENT_STATISTICAL, 0.90)
        
        return (None, 0.0)
    
    def _calculate_vector_similarity(self, doc) -> Tuple[str, float]:
        """Calcula similaridad vectorial con intenciones"""
        if not doc.has_vector:
            return (INTENT_GENERAL, 0.5)
        
        question_vector = doc.vector
        similarities = {}
        
        for intent, intent_vector in self.intent_vectors.items():
            similarity = self._cosine_similarity(question_vector, intent_vector)
            similarities[intent] = similarity
        
        best_intent = max(similarities, key=similarities.get)
        confidence = similarities[best_intent]
        
        return (best_intent, confidence)
    
    def classify(self, text: str) -> Tuple[str, float]:
        """Clasifica con prioridad en patrones"""
        doc = self.nlp(text.lower())
        text_lower = text.lower()
        
        # PASO 1: Detecci칩n directa con regex
        is_stats, confidence = self._check_regex_patterns(text_lower)
        if is_stats:
            return (INTENT_STATISTICAL, confidence)
        
        # PASO 2: Verificar patrones sint치cticos
        intent, confidence = self._check_syntactic_patterns(doc, text_lower)
        if intent:
            return (intent, confidence)
        
        # PASO 3: Palabras clave directas
        intent, confidence = self._check_keyword_match(text_lower)
        if intent:
            return (intent, confidence)
        
        # PASO 4: Similaridad vectorial
        intent, confidence = self._calculate_vector_similarity(doc)
        
        # PASO 5: Ajustes contextuales
        intent, confidence = self._contextual_adjustments(doc, intent, confidence, text_lower)
        
        if confidence < 0.6:
            return (INTENT_GENERAL, confidence)
        
        print(f"游꿢 Intenci칩n: {intent} (confianza: {confidence:.2f})")
        return (intent, confidence)
    
    def _cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """Similaridad coseno"""
        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return dot_product / (norm1 * norm2)
    
    def _contextual_adjustments(self, doc, intent: str, confidence: float, text_lower: str) -> Tuple[str, float]:
        """Ajustes finales seg칰n contexto"""
        
        # Saludo corto
        if len(doc) <= 3:
            greeting_words = ['hola', 'hey', 'buenas', 'buenos']
            if any(token.text in greeting_words for token in doc):
                return (INTENT_GREETING, 0.95)
        
        # Forzar estad칤sticas si menciona palabras clave
        if any(word in text_lower for word in STATS_KEYWORDS):
            # Solo si NO es claramente estructura
            if 'columnas' not in text_lower or LITERAL_ESTADISTICAS in text_lower:  # Usando constante
                print("游댢 Ajuste: Forzando STATISTICAL")
                return (INTENT_STATISTICAL, 0.95)
        
        # Solo estructura si menciona columnas sin estad칤sticas
        if any(word in text_lower for word in STRUCTURE_KEYWORDS):
            if not any(word in text_lower for word in STATS_KEYWORDS):
                return (INTENT_STRUCTURE, min(confidence + 0.1, 0.95))
        
        return (intent, confidence)
    
    def get_intent_details(self, text: str) -> Dict:
        """An치lisis completo de la intenci칩n"""
        intent, confidence = self.classify(text)
        doc = self.nlp(text)
        
        return {
            'intent': intent,
            'confidence': confidence,
            'entities': [{'text': ent.text, 'label': ent.label_} for ent in doc.ents],
            'key_tokens': [token.text for token in doc if token.pos_ in ['NOUN', 'VERB'] and not token.is_stop][:5],
            'has_negation': any(token.dep_ == 'neg' for token in doc),
            'is_question': doc[0].text.lower() in ['qu칠', 'cu치l', 'c칩mo', 'd칩nde', 'cu치ndo', 'qui칠n']
        }


# Instancia global
intent_classifier = IntentClassifier()
