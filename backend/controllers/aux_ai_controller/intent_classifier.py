# controllers/aux_ai_controller/intent_classifier.py
import spacy
from spacy.matcher import Matcher
from typing import Dict, Tuple
import numpy as np
import re


class IntentClassifier:
    """Clasificador mejorado con detecci√≥n robusta de estad√≠sticas"""
    
    def __init__(self):
        try:
            self.nlp = spacy.load("es_core_news_lg")
            print("‚úÖ Modelo spaCy large cargado")
        except OSError:
            import subprocess
            subprocess.run(["python", "-m", "spacy", "download", "es_core_news_lg"])
            self.nlp = spacy.load("es_core_news_lg")
        
        self.intent_examples = {
            'greeting': [
                "hola", "buenos d√≠as", "buenas tardes", "hey", "qu√© tal"
            ],
            'structure_analysis': [
                "qu√© columnas tiene", "muestra las columnas", 
                "cu√°les son los campos", "estructura del archivo",
                "columnas disponibles", "listar columnas"
            ],
            'statistical': [
                # ‚úÖ M√ÅS VARIACIONES ESPEC√çFICAS
                "realiza estad√≠sticas", "genera estad√≠sticas", "generame estad√≠sticas",
                "calcula estad√≠sticas", "dame estad√≠sticas", "estad√≠sticas del archivo",
                "hacer estad√≠sticas", "obtener estad√≠sticas", "mostrar estad√≠sticas",
                "an√°lisis estad√≠stico", "resumen estad√≠stico", "estad√≠sticas descriptivas",
                "cu√°l es el promedio", "calcula la media", "dame la mediana",
                "suma total", "contar registros", "m√°ximo y m√≠nimo",
                "desviaci√≥n est√°ndar", "frecuencias", "distribuci√≥n",
                "m√©tricas", "an√°lisis num√©rico", "valores estad√≠sticos"
            ],
            'filtering': [
                "filtrar por", "buscar donde", "encontrar registros"
            ],
            'temporal': [
                "an√°lisis temporal", "tendencia", "serie de tiempo"
            ],
            'help': [
                "c√≥mo puedo", "ay√∫dame", "explica c√≥mo"
            ],
            'export': [
                "exportar datos", "descargar archivo"
            ],
            'comparison': [
                "comparar", "diferencia entre"
            ]
        }
        
        self.intent_vectors = self._create_intent_vectors()
        self._setup_patterns()
    
    def _create_intent_vectors(self) -> Dict[str, np.ndarray]:
        """Crea vectores para cada intenci√≥n"""
        intent_vectors = {}
        
        for intent, examples in self.intent_examples.items():
            vectors = []
            for example in examples:
                doc = self.nlp(example)
                if doc.has_vector:
                    vectors.append(doc.vector)
            
            if vectors:
                intent_vectors[intent] = np.mean(vectors, axis=0)
        
        return intent_vectors
    
    def _setup_patterns(self):
        """Configura patrones mejorados"""
        self.matcher = Matcher(self.nlp.vocab)
        
        # ‚úÖ PATR√ìN 1: [VERBO] + estad√≠sticas
        pattern_stats_verb = [
            {"LEMMA": {"IN": [
                "realizar", "generar", "calcular", "obtener", 
                "hacer", "dame", "mostrar", "crear", "producir"
            ]}},
            {"OP": "*", "IS_PUNCT": False},  # Palabras intermedias
            {"LOWER": {"IN": ["estad√≠sticas", "estad√≠stica", "estadistico", "estad√≠sticos"]}}
        ]
        self.matcher.add("STATS_VERB", [pattern_stats_verb])
        
        # ‚úÖ PATR√ìN 2: estad√≠sticas + de/del + [archivo/datos]
        pattern_stats_of = [
            {"LOWER": {"IN": ["estad√≠sticas", "estad√≠stica"]}},
            {"LOWER": {"IN": ["de", "del", "para"]}},
            {"OP": "*"}
        ]
        self.matcher.add("STATS_OF", [pattern_stats_of])
        
        # ‚úÖ PATR√ìN 3: Solo "estad√≠sticas" en consulta corta
        pattern_stats_simple = [
            {"LOWER": {"IN": ["estad√≠sticas", "estad√≠stica", "estadisticos"]}}
        ]
        self.matcher.add("STATS_SIMPLE", [pattern_stats_simple])
        
        # Patr√≥n para estructura (solo si menciona "columnas" o "campos")
        pattern_structure = [
            {"LEMMA": {"IN": ["mostrar", "listar", "ver", "ense√±ar"]}},
            {"OP": "*"},
            {"LOWER": {"IN": ["columnas", "campos", "estructura"]}}
        ]
        self.matcher.add("STRUCTURE_PATTERN", [pattern_structure])
    
    def classify(self, text: str) -> Tuple[str, float]:
        """Clasifica con prioridad en patrones"""
        doc = self.nlp(text.lower())
        text_lower = text.lower()
        
        # ‚úÖ PASO 1: DETECCI√ìN DIRECTA CON REGEX (M√ÅS R√ÅPIDO Y PRECISO)
        stats_patterns = [
            r'\b(genera|realiza|calcula|dame|obtener|hacer|muestra|crea)\s+(?:las?\s+)?estad[i√≠]sticas?\b',
            r'\bestad[i√≠]sticas?\s+(?:de|del|para)\b',
            r'\bestad[i√≠]sticas?\s+(?:descriptivas?|generales?)\b',
            r'\bestad[i√≠]sticas?\b.*\barchivo\b',
            r'\barchivo\b.*\bestad[i√≠]sticas?\b'
        ]
        
        for pattern in stats_patterns:
            if re.search(pattern, text_lower, re.IGNORECASE):
                print("üéØ ESTAD√çSTICAS detectadas por regex")
                return ('statistical', 0.98)
        
        # ‚úÖ PASO 2: VERIFICAR PATRONES SINT√ÅCTICOS
        matches = self.matcher(doc)
        if matches:
            match_names = [self.nlp.vocab.strings[match_id] for match_id, start, end in matches]
            
            # Prioridad m√°xima: patrones de estad√≠sticas
            if any(name in match_names for name in ["STATS_VERB", "STATS_OF", "STATS_SIMPLE"]):
                print("üéØ ESTAD√çSTICAS detectadas por patr√≥n sint√°ctico")
                return ('statistical', 0.97)
            
            # Solo estructura si menciona columnas Y NO estad√≠sticas
            if "STRUCTURE_PATTERN" in match_names and "estad" not in text_lower:
                print("üéØ ESTRUCTURA detectada")
                return ('structure_analysis', 0.95)
        
        # ‚úÖ PASO 3: PALABRAS CLAVE DIRECTAS
        stats_keywords = [
            'estad√≠sticas', 'estad√≠stica', 'estadistico', 'estad√≠sticos',
            'promedio', 'media', 'mediana', 'moda',
            'suma', 'total', 'conteo',
            'm√°ximo', 'm√≠nimo', 'rango',
            'desviaci√≥n', 'varianza', 'frecuencia',
            'distribuci√≥n', 'percentil'
        ]
        
        # Contar cu√°ntas palabras clave aparecen
        stats_count = sum(1 for keyword in stats_keywords if keyword in text_lower)
        
        if stats_count >= 1:
            # Verificar que NO sea solo consulta de estructura
            structure_keywords = ['columnas', 'campos', 'estructura']
            structure_count = sum(1 for keyword in structure_keywords if keyword in text_lower)
            
            if structure_count == 0 or stats_count > structure_count:
                print(f"üéØ ESTAD√çSTICAS detectadas ({stats_count} palabras clave)")
                return ('statistical', 0.90)
        
        # PASO 4: Similaridad vectorial
        if not doc.has_vector:
            return ('general', 0.5)
        
        question_vector = doc.vector
        similarities = {}
        
        for intent, intent_vector in self.intent_vectors.items():
            similarity = self._cosine_similarity(question_vector, intent_vector)
            similarities[intent] = similarity
        
        best_intent = max(similarities, key=similarities.get)
        confidence = similarities[best_intent]
        
        # PASO 5: Ajustes contextuales
        best_intent, confidence = self._contextual_adjustments(doc, best_intent, confidence, text_lower)
        
        if confidence < 0.6:
            return ('general', confidence)
        
        print(f"üéØ Intenci√≥n: {best_intent} (confianza: {confidence:.2f})")
        return (best_intent, confidence)
    
    def _cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """Similaridad coseno"""
        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return dot_product / (norm1 * norm2)
    
    def _contextual_adjustments(self, doc, intent: str, confidence: float, text_lower: str) -> Tuple[str, float]:
        """Ajustes finales"""
        
        # Saludo corto
        if len(doc) <= 3:
            greeting_words = ['hola', 'hey', 'buenas', 'buenos']
            if any(token.text in greeting_words for token in doc):
                return ('greeting', 0.95)
        
        # ‚úÖ FORZAR ESTAD√çSTICAS SI MENCIONA PALABRAS CLAVE
        stats_words = [
            'estad√≠sticas', 'estad√≠stica', 'promedio', 'media', 'mediana',
            'suma', 'total', 'm√°ximo', 'm√≠nimo', 'desviaci√≥n', 'frecuencia'
        ]
        
        if any(word in text_lower for word in stats_words):
            # Solo si NO es claramente estructura
            if 'columnas' not in text_lower or 'estad√≠sticas' in text_lower:
                print("üîÑ Ajuste: Forzando STATISTICAL")
                return ('statistical', 0.95)
        
        # Solo estructura si menciona columnas sin estad√≠sticas
        structure_words = ['columnas', 'campos', 'estructura']
        if any(word in text_lower for word in structure_words):
            if not any(word in text_lower for word in stats_words):
                return ('structure_analysis', min(confidence + 0.1, 0.95))
        
        return (intent, confidence)
    
    def get_intent_details(self, text: str) -> Dict:
        """An√°lisis completo"""
        intent, confidence = self.classify(text)
        doc = self.nlp(text)
        
        return {
            'intent': intent,
            'confidence': confidence,
            'entities': [{'text': ent.text, 'label': ent.label_} for ent in doc.ents],
            'key_tokens': [token.text for token in doc if token.pos_ in ['NOUN', 'VERB'] and not token.is_stop][:5],
            'has_negation': any(token.dep_ == 'neg' for token in doc),
            'is_question': doc[0].text.lower() in ['qu√©', 'cu√°l', 'c√≥mo', 'd√≥nde', 'cu√°ndo', 'qui√©n']
        }


# Instancia global
intent_classifier = IntentClassifier()
