# controllers/ai_controller.py - CORRECCIÃ“N PARA LEER METADATA_CACHE
import google.generativeai as genai
import asyncio
import os
import json
import glob
from typing import Dict, Any, Optional, List
from config import GEMINI_API_KEY
from controllers.files_controllers.storage_manager import FileStorageManager
from services.duckdb_service.duckdb_service import duckdb_service

genai.configure(api_key=GEMINI_API_KEY)
model = genai.GenerativeModel('gemini-1.5-flash')

class AIController:
    def __init__(self):
        self.storage_manager = FileStorageManager()
        self.max_sample_rows = 5
        self.max_context_length = 8000
        
        # âœ… RUTAS A LAS CARPETAS DE METADATOS
        self.metadata_cache_path = "metadata_cache"
        self.parquet_cache_path = "parquet_cache"
    
    async def ask_ai(self, request) -> Dict[str, Any]:
        """Procesa consulta al asistente IA - VERSIÃ“N CORREGIDA"""
        try:
            print(f"ğŸ¤– Procesando consulta AI: {request.question[:100]}...")
            
            # âœ… CONSTRUIR CONTEXTO DESDE METADATA_CACHE
            context = await self._build_comprehensive_context_async(request.file_context)
            
            # Determinar tipo de consulta
            query_type = self._analyze_query_type(request.question)
            
            # Construir prompt
            prompt = self._build_smart_prompt(context, request.question, query_type)
            
            print(f"ğŸ“ Prompt generado ({len(prompt)} chars)")
            
            # Generar respuesta
            response = await self._generate_ai_response_async(prompt)
            
            # Procesar respuesta
            processed_response = self._process_ai_response(
                response, 
                request.file_context, 
                query_type
            )
            
            return {
                "success": True,
                "response": processed_response,
                "context_type": "file_specific" if request.file_context else "general",
                "query_type": query_type,
                "file_context": request.file_context
            }
            
        except Exception as e:
            print(f"âŒ Error en AI Controller: {e}")
            return {
                "success": False,
                "response": f"Lo siento, ocurriÃ³ un error al procesar tu consulta: {str(e)}",
                "error": str(e)
            }
    
    async def _build_comprehensive_context_async(self, file_id: Optional[str] = None) -> str:
        """Construye contexto desde metadata_cache"""
        try:
            context_parts = []
            
            # âœ… LEER METADATOS DESDE METADATA_CACHE
            available_files = await self._get_available_files_from_metadata()
            
            if not available_files:
                return """
No hay archivos cargados actualmente. 

ğŸ“ **Para empezar:**
1. Ve a la secciÃ³n "Subir Archivos" (/technical-note/upload)
2. Selecciona un archivo CSV o Excel
3. Una vez cargado, podrÃ¡s hacer consultas especÃ­ficas sobre tus datos

ğŸ’¡ **Tipos de anÃ¡lisis disponibles:**
â€¢ AnÃ¡lisis estadÃ­stico bÃ¡sico
â€¢ Filtrado y bÃºsqueda de datos
â€¢ AnÃ¡lisis temporal
â€¢ Reportes de nota tÃ©cnica (inasistentes por edad)
â€¢ Cruce de archivos (VLOOKUP)
"""
            
            if file_id:
                # âœ… CONTEXTO ESPECÃFICO DEL ARCHIVO
                context_parts.append("=== ARCHIVO SELECCIONADO ===")
                file_context = await self._build_file_specific_context_from_metadata(file_id, available_files)
                context_parts.append(file_context)
            else:
                # âœ… CONTEXTO GENERAL DE TODOS LOS ARCHIVOS
                context_parts.append("=== ARCHIVOS DISPONIBLES ===")
                general_context = self._build_general_context_from_metadata(available_files)
                context_parts.append(general_context)
            
            # Capacidades del sistema
            context_parts.append("\n=== CAPACIDADES DEL SISTEMA ===")
            context_parts.append(self._get_system_capabilities())
            
            full_context = "\n".join(context_parts)
            
            # Limitar longitud
            if len(full_context) > self.max_context_length:
                full_context = full_context[:self.max_context_length] + "...\n[Contexto truncado]"
            
            return full_context
            
        except Exception as e:
            print(f"âŒ Error construyendo contexto: {e}")
            return "Contexto no disponible debido a un error interno."
    
    async def _get_available_files_from_metadata(self) -> List[Dict[str, Any]]:
        """Lee todos los archivos de metadatos disponibles"""
        try:
            available_files = []
            
            if not os.path.exists(self.metadata_cache_path):
                print(f"âš ï¸ Carpeta {self.metadata_cache_path} no encontrada")
                return []
            
            # âœ… BUSCAR TODOS LOS JSON EN METADATA_CACHE
            metadata_files = glob.glob(os.path.join(self.metadata_cache_path, "*.json"))
            
            for metadata_file in metadata_files:
                try:
                    with open(metadata_file, 'r', encoding='utf-8') as f:
                        metadata = json.load(f)
                    
                    # Agregar informaciÃ³n del archivo
                    file_info = {
                        'file_id': metadata.get('file_id', os.path.basename(metadata_file).replace('.json', '')),
                        'original_name': metadata.get('original_name', 'Desconocido'),
                        'extension': metadata.get('extension', 'csv'),
                        'columns': metadata.get('columns', []),
                        'total_rows': metadata.get('total_rows', 0),
                        'file_size_mb': metadata.get('original_size_mb', 0),
                        'cached_at': metadata.get('cached_at', ''),
                        'parquet_path': metadata.get('parquet_path', ''),
                        'compression_ratio': metadata.get('compression_ratio', 0)
                    }
                    
                    available_files.append(file_info)
                    
                except Exception as file_error:
                    print(f"âš ï¸ Error leyendo {metadata_file}: {file_error}")
                    continue
            
            print(f"ğŸ“ Encontrados {len(available_files)} archivos en metadata_cache")
            return available_files
            
        except Exception as e:
            print(f"âŒ Error obteniendo archivos: {e}")
            return []
    
    async def _build_file_specific_context_from_metadata(self, file_id: str, available_files: List[Dict]) -> str:
        """Construye contexto especÃ­fico usando metadatos"""
        try:
            # Buscar archivo especÃ­fico
            file_info = None
            for f in available_files:
                if f['file_id'] == file_id or f['original_name'] == file_id:
                    file_info = f
                    break
            
            if not file_info:
                return f"âŒ Archivo {file_id} no encontrado en los metadatos."
            
            context_parts = []
            
            # âœ… INFORMACIÃ“N DETALLADA DEL ARCHIVO
            context_parts.append(f"ğŸ“„ **Archivo:** {file_info['original_name']}")
            context_parts.append(f"ğŸ“Š **Tipo:** {file_info['extension'].upper()}")
            context_parts.append(f"ğŸ“ˆ **Filas:** {file_info['total_rows']:,} registros")
            context_parts.append(f"ğŸ“‹ **Columnas:** {len(file_info['columns'])} campos")
            
            if file_info['file_size_mb'] > 0:
                context_parts.append(f"ğŸ’¾ **TamaÃ±o:** {file_info['file_size_mb']:.2f} MB")
            
            if file_info['compression_ratio'] > 0:
                context_parts.append(f"ğŸ—œï¸ **CompresiÃ³n:** {file_info['compression_ratio']:.1f}%")
            
            # âœ… ESTRUCTURA DE COLUMNAS
            columns = file_info['columns']
            if columns:
                context_parts.append(f"\n**ğŸ“‹ ESTRUCTURA DE DATOS:**")
                for i, col in enumerate(columns[:25], 1):  # MÃ¡ximo 25 columnas
                    context_parts.append(f"  {i:2}. {col}")
                
                if len(columns) > 25:
                    context_parts.append(f"  ... y {len(columns)-25} columnas mÃ¡s")
            
            # âœ… ESTADÃSTICAS ADICIONALES SI ESTÃN DISPONIBLES
            if file_info['cached_at']:
                try:
                    from datetime import datetime
                    cached_time = datetime.fromisoformat(file_info['cached_at'].replace('Z', '+00:00'))
                    context_parts.append(f"\nâ° **Procesado:** {cached_time.strftime('%Y-%m-%d %H:%M')}")
                except:
                    pass
            
            # âœ… INTENTAR OBTENER MUESTRA DE DATOS
            try:
                sample_data = await self._get_sample_data_async(file_info['file_id'])
                if sample_data:
                    context_parts.append(f"\n**ğŸ” MUESTRA DE DATOS:**")
                    context_parts.append(sample_data)
            except Exception as sample_error:
                print(f"âš ï¸ Error obteniendo muestra: {sample_error}")
            
            return "\n".join(context_parts)
            
        except Exception as e:
            print(f"âŒ Error en contexto especÃ­fico: {e}")
            return f"Error obteniendo informaciÃ³n del archivo {file_id}"
    
    def _build_general_context_from_metadata(self, available_files: List[Dict]) -> str:
        """Construye contexto general desde metadatos"""
        try:
            if not available_files:
                return "No hay archivos cargados actualmente."
            
            context_parts = []
            context_parts.append(f"ğŸ“ **Total de archivos:** {len(available_files)}")
            
            # âœ… ESTADÃSTICAS GENERALES
            total_rows = sum(f['total_rows'] for f in available_files)
            total_size = sum(f['file_size_mb'] for f in available_files)
            
            context_parts.append(f"ğŸ“Š **Registros totales:** {total_rows:,}")
            if total_size > 0:
                context_parts.append(f"ğŸ’¾ **TamaÃ±o total:** {total_size:.1f} MB")
            
            # âœ… LISTA DE ARCHIVOS DISPONIBLES
            context_parts.append(f"\n**ğŸ“‹ ARCHIVOS DISPONIBLES:**")
            for i, file_info in enumerate(available_files, 1):
                file_summary = (
                    f"{i:2}. **{file_info['original_name']}** "
                    f"({file_info['extension'].upper()}) - "
                    f"{len(file_info['columns'])} columnas, "
                    f"{file_info['total_rows']:,} filas"
                )
                context_parts.append(f"   {file_summary}")
            
            return "\n".join(context_parts)
            
        except Exception as e:
            print(f"âŒ Error en contexto general: {e}")
            return "Error obteniendo informaciÃ³n general de archivos."
    
    # âœ… MANTENER OTROS MÃ‰TODOS EXISTENTES
    async def _generate_ai_response_async(self, prompt: str) -> str:
        """Genera respuesta AI de forma async"""
        try:
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: model.generate_content(prompt)
            )
            return response.text
        except Exception as e:
            return f"Error generando respuesta: {str(e)}"
    
    async def _get_sample_data_async(self, file_id: str) -> str:
        """Obtiene muestra de datos usando DuckDB"""
        try:
            loop = asyncio.get_event_loop()
            
            def get_sample():
                sample_query = f"SELECT * FROM '{file_id}' LIMIT {self.max_sample_rows}"
                return duckdb_service.conn.execute(sample_query).fetchdf()
            
            result = await loop.run_in_executor(None, get_sample)
            
            if not result.empty:
                markdown_table = result.to_markdown(index=False, tablefmt="grid")
                return f"``````"
            
            return "No se pudo obtener muestra de datos"
            
        except Exception as e:
            print(f"âš ï¸ Error obteniendo muestra: {e}")
            return "Muestra no disponible"
    
    def _analyze_query_type(self, question: str) -> str:
        """Analiza tipo de consulta"""
        question_lower = question.lower()
        
        if any(word in question_lower for word in ['hola', 'hello', 'hi', 'buenos dÃ­as', 'buenas tardes']):
            return 'greeting'
        elif any(word in question_lower for word in ['estadÃ­stica', 'promedio', 'suma', 'contar']):
            return 'statistical'
        elif any(word in question_lower for word in ['filtrar', 'buscar', 'encontrar']):
            return 'filtering'
        elif any(word in question_lower for word in ['columna', 'campo', 'estructura']):
            return 'schema'
        elif any(word in question_lower for word in ['temporal', 'tiempo', 'fecha']):
            return 'temporal'
        elif any(word in question_lower for word in ['nota tÃ©cnica', 'inasistentes']):
            return 'technical_note'
        elif any(word in question_lower for word in ['ayuda', 'cÃ³mo', 'tutorial']):
            return 'help'
        else:
            return 'general'
    
    def _build_smart_prompt(self, context: str, question: str, query_type: str) -> str:
        """Construye prompt inteligente"""
        
        greeting_context = ""
        if query_type == 'greeting':
            greeting_context = """
INSTRUCCIONES PARA SALUDO:
- Responde de manera muy Breve
- Responde de manera amigable y profesional
- Menciona brevemente los archivos disponibles si los hay
- Invita al usuario a hacer preguntas especÃ­ficas sobre los datos
- Sugiere tipos de anÃ¡lisis que puede realizar
"""
        
        base_prompt = f"""
Eres un asistente especializado en anÃ¡lisis de datos que ayuda a usuarios a entender y analizar sus archivos de datos.

CONTEXTO DE DATOS DISPONIBLES:
{context}

{greeting_context}

PREGUNTA DEL USUARIO: {question}

RESPUESTA:
Proporciona una respuesta Ãºtil, especÃ­fica y accionable. Si sugieres anÃ¡lisis, explica cÃ³mo usar las herramientas disponibles.
"""
        
        return base_prompt
    
    def _process_ai_response(self, ai_response: str, file_context: Optional[str], query_type: str) -> str:
        """Procesa respuesta AI"""
        processed_response = ai_response
        
        if query_type == 'greeting':
            processed_response += "\n\nğŸ’¡ **Â¿QuÃ© puedo hacer por ti?**\nâ€¢ Analizar estructura de datos\nâ€¢ Generar estadÃ­sticas\nâ€¢ Ayudar con filtros y bÃºsquedas\nâ€¢ Explicar cÃ³mo usar las herramientas"
        elif query_type == 'statistical' and file_context:
            processed_response += "\n\nğŸ’¡ **Sugerencia:** Usa la secciÃ³n 'AnÃ¡lisis' para estadÃ­sticas detalladas."
        elif query_type == 'technical_note':
            processed_response += "\n\nğŸ’¡ **Sugerencia:** Usa 'Nota TÃ©cnica' para reportes de inasistentes."
        elif not file_context:
            processed_response += "\n\nğŸ“ **Tip:** Selecciona un archivo especÃ­fico para anÃ¡lisis mÃ¡s detallados."
        
        return processed_response
    
    def _get_system_capabilities(self) -> str:
        """Describe capacidades del sistema"""
        return """
ğŸ”§ **AnÃ¡lisis disponibles:**
â€¢ Filtrado y bÃºsqueda de datos
â€¢ EstadÃ­sticas y agregaciones  
â€¢ AnÃ¡lisis temporal
â€¢ AnÃ¡lisis de nota tÃ©cnica (inasistentes)
â€¢ Cruce de archivos (VLOOKUP)
â€¢ ExportaciÃ³n de resultados

ğŸ’¡ **Puedes preguntar sobre:**
â€¢ Estructura y contenido de datos
â€¢ Patrones y tendencias
â€¢ EstadÃ­sticas especÃ­ficas
â€¢ CÃ³mo usar las herramientas disponibles
"""

# âœ… INSTANCIA GLOBAL
ai_controller = AIController()
