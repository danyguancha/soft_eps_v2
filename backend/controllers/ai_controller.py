# controllers/ai_controller.py - ORQUESTADOR CON NLP AUTOM√ÅTICO
import google.generativeai as genai
import asyncio
from typing import Dict, Any
from config import GEMINI_API_KEY

# Importar componentes NLP automatizados
from controllers.aux_ai_controller.context_builder import context_builder
from controllers.aux_ai_controller.query_analyzer import query_analyzer
from controllers.aux_ai_controller.prompt_builder import prompt_builder
from controllers.aux_ai_controller.response_processor import response_processor
from controllers.aux_ai_controller.conversation_manager import conversation_manager
from controllers.aux_ai_controller.sql_executor import sql_executor
from controllers.aux_ai_controller.intent_classifier import intent_classifier

genai.configure(api_key=GEMINI_API_KEY)
model = genai.GenerativeModel('gemini-2.0-flash-exp')


class AIController:
    """
    ü§ñ Orquestador principal del asistente AI con NLP autom√°tico
    
    Caracter√≠sticas:
    - Detecci√≥n autom√°tica de intenciones con spaCy
    - An√°lisis sem√°ntico avanzado
    - C√°lculo de estad√≠sticas en tiempo real
    - Gesti√≥n de conversaciones con contexto
    - Respuestas inteligentes y contextuales
    """
    
    def __init__(self):
        print("üöÄ Inicializando AIController con NLP autom√°tico...")
        self.context_builder = context_builder
        self.query_analyzer = query_analyzer
        self.prompt_builder = prompt_builder
        self.response_processor = response_processor
        self.conversation_manager = conversation_manager
        self.sql_executor = sql_executor
        self.intent_classifier = intent_classifier
        print("‚úÖ AIController listo")
    
    async def ask_ai(self, request) -> Dict[str, Any]:
        """
        Procesa consulta del usuario con an√°lisis NLP autom√°tico
        
        Flujo:
        1. An√°lisis NLP autom√°tico (intenci√≥n, entidades, keywords)
        2. Detecci√≥n autom√°tica de archivo objetivo
        3. C√°lculo de estad√≠sticas si es necesario
        4. Construcci√≥n de contexto enriquecido
        5. Generaci√≥n de respuesta con Gemini
        6. Post-procesamiento y mejora de respuesta
        """
        try:
            session_id = getattr(request, 'session_id', 'default_session')
            
            print(f"\n{'='*60}")
            print(f"ü§ñ NUEVA CONSULTA")
            print(f"{'='*60}")
            print(f"üìù Pregunta: {request.question}")
            print(f"üîë Session: {session_id}")
            
            # PASO 1: Guardar pregunta del usuario en el historial
            self.conversation_manager.add_message(session_id, 'user', request.question)
            
            # PASO 2: Obtener archivos disponibles
            available_files = await self.context_builder.get_available_files()
            print(f"üìÅ Archivos disponibles: {len(available_files)}")
            for f in available_files:
                print(f"   ‚Ä¢ {f['original_name']} ({f['total_rows']:,} filas)")
            
            # PASO 3: üß† AN√ÅLISIS NLP AUTOM√ÅTICO
            print(f"\nüß† Analizando consulta con NLP...")
            query_analysis = self.query_analyzer.analyze(request.question, available_files)
            
            print(f"‚úÖ Intenci√≥n detectada: {query_analysis['type']} (confianza: {query_analysis.get('confidence', 0)*100:.1f}%)")
            print(f"   Acci√≥n: {query_analysis['intent']}")
            print(f"   Requiere archivo: {query_analysis['requires_file']}")
            
            if query_analysis.get('keywords'):
                print(f"   Palabras clave: {', '.join(query_analysis['keywords'][:5])}")
            
            if query_analysis.get('numerical_operations'):
                print(f"   Operaciones: {', '.join(query_analysis['numerical_operations'])}")
            
            # PASO 4: üéØ DETERMINACI√ìN INTELIGENTE DE ARCHIVO OBJETIVO
            print(f"\nüéØ Determinando archivo objetivo...")
            
            # Prioridad 1: Archivo detectado autom√°ticamente por NLP
            target_file = query_analysis.get('target_file')
            
            # Prioridad 2: Archivo del contexto conversacional
            if not target_file:
                target_file = self.conversation_manager.extract_file_context_nlp(
                    session_id, 
                    request.question, 
                    available_files,
                    query_analysis
                )
            
            # Prioridad 3: Archivo expl√≠cito del request
            if not target_file:
                target_file = request.file_context
            
            # Prioridad 4: Si solo hay un archivo, usarlo
            if not target_file and len(available_files) == 1:
                target_file = available_files[0]['file_id']
                print(f"   ‚ÑπÔ∏è Auto-seleccionando √∫nico archivo disponible")
            
            if target_file:
                file_name = next((f['original_name'] for f in available_files if f['file_id'] == target_file), target_file)
                print(f"‚úÖ Archivo objetivo: {file_name}")
            else:
                print(f"‚ÑπÔ∏è No se requiere archivo espec√≠fico")
            
            # PASO 5: üìä C√ÅLCULO AUTOM√ÅTICO DE ESTAD√çSTICAS
            calculated_stats = None
            
            if query_analysis['type'] == 'statistical' and target_file:
                print(f"\nüìä Calculando estad√≠sticas en tiempo real...")
                
                # Buscar informaci√≥n del archivo
                file_info = next((f for f in available_files if f['file_id'] == target_file or target_file in f['original_name']), None)
                
                if file_info and file_info.get('columns'):
                    parquet_path = file_info.get('parquet_path')
                    
                    if parquet_path:
                        print(f"   üìÇ Usando: {parquet_path}")
                        print(f"   üî¢ Columnas a analizar: {len(file_info['columns'])}")
                        
                        calculated_stats = await self.sql_executor.calculate_statistics(
                            file_info['file_id'],
                            file_info['columns'],
                            parquet_path
                        )
                        
                        if calculated_stats:
                            num_numeric = len(calculated_stats.get('numeric', {}))
                            num_categorical = len(calculated_stats.get('categorical', {}))
                            print(f"   ‚úÖ Estad√≠sticas calculadas:")
                            print(f"      ‚Ä¢ {num_numeric} columnas num√©ricas analizadas")
                            print(f"      ‚Ä¢ {num_categorical} columnas categ√≥ricas analizadas")
                        else:
                            print(f"   ‚ö†Ô∏è No se pudieron calcular estad√≠sticas")
                    else:
                        print(f"   ‚ö†Ô∏è Parquet no disponible para {target_file}")
                else:
                    print(f"   ‚ö†Ô∏è Informaci√≥n del archivo no disponible")
            
            # PASO 6: üìù CONSTRUCCI√ìN DE CONTEXTO ENRIQUECIDO
            print(f"\nüìù Construyendo contexto...")
            context = await self.context_builder.build_context(target_file)
            
            # Agregar estad√≠sticas calculadas al contexto
            if calculated_stats:
                context += "\n\n" + "="*60
                context += "\nüî¢ ESTAD√çSTICAS CALCULADAS EN TIEMPO REAL\n"
                context += "="*60 + "\n"
                context += self._format_statistics(calculated_stats)
                print(f"   ‚úÖ Contexto enriquecido con estad√≠sticas reales")
            
            # PASO 7: üí¨ CONTEXTO CONVERSACIONAL
            conversation_context = self.conversation_manager.build_conversation_context(session_id)
            
            if conversation_context:
                intent_pattern = self.conversation_manager.get_intent_pattern(session_id)
                print(f"   üí¨ Contexto conversacional disponible")
                print(f"   üìä Patr√≥n de intenciones: {' ‚Üí '.join(intent_pattern[-3:])}")
            
            # PASO 8: üé® CONSTRUCCI√ìN DE PROMPT DIN√ÅMICO
            print(f"\nüé® Generando prompt optimizado...")
            prompt = self.prompt_builder.build(
                context, 
                request.question, 
                query_analysis,
                conversation_context
            )
            
            prompt_preview = prompt[:200].replace('\n', ' ')
            print(f"   Preview: {prompt_preview}...")
            
            # PASO 9: ü§ñ GENERACI√ìN DE RESPUESTA CON GEMINI
            print(f"\nü§ñ Consultando a Gemini...")
            ai_response = await self._generate_response(prompt)
            
            response_preview = ai_response[:150].replace('\n', ' ')
            print(f"   ‚úÖ Respuesta recibida: {response_preview}...")
            
            # PASO 10: üîß POST-PROCESAMIENTO DE RESPUESTA
            print(f"\nüîß Post-procesando respuesta...")
            final_response = self.response_processor.process(
                ai_response,
                query_analysis,
                target_file
            )
            
            # PASO 11: üíæ GUARDAR EN HISTORIAL
            self.conversation_manager.add_message(
                session_id, 
                'assistant', 
                final_response,
                {
                    'query_type': query_analysis['type'],
                    'target_file': target_file,
                    'confidence': query_analysis.get('confidence'),
                    'has_stats': bool(calculated_stats)
                }
            )
            
            print(f"\n{'='*60}")
            print(f"‚úÖ CONSULTA COMPLETADA EXITOSAMENTE")
            print(f"{'='*60}\n")
            
            return {
                "success": True,
                "response": final_response,
                "query_type": query_analysis['type'],
                "confidence": query_analysis.get('confidence'),
                "target_file": target_file,
                "session_id": session_id,
                "has_calculated_stats": bool(calculated_stats),
                "intent_action": query_analysis['intent'],
                "keywords_detected": query_analysis.get('keywords', []),
                "entities_detected": query_analysis.get('entities', [])
            }
            
        except Exception as e:
            import traceback
            error_trace = traceback.format_exc()
            
            print(f"\n{'='*60}")
            print(f"‚ùå ERROR EN CONSULTA")
            print(f"{'='*60}")
            print(f"Error: {str(e)}")
            print(f"Traceback:\n{error_trace}")
            print(f"{'='*60}\n")
            
            return {
                "success": False,
                "response": f"Lo siento, ocurri√≥ un error al procesar tu consulta. Por favor, intenta reformularla o contacta al administrador.",
                "error": str(e),
                "traceback": error_trace
            }
    
    def _format_statistics(self, stats: Dict[str, Any]) -> str:
        """
        Formatea estad√≠sticas calculadas para el contexto del AI
        
        Formato optimizado para que Gemini pueda interpretar y presentar
        """
        formatted = []
        
        # Estad√≠sticas num√©ricas
        if 'numeric' in stats and stats['numeric']:
            formatted.append("\nüìä COLUMNAS NUM√âRICAS:\n")
            
            for col, values in stats['numeric'].items():
                formatted.append(f"**{col}**")
                formatted.append(f"  ‚Ä¢ Promedio: {values['promedio']:.2f}")
                formatted.append(f"  ‚Ä¢ Mediana: {values['mediana']:.2f}")
                formatted.append(f"  ‚Ä¢ M√≠nimo: {values['minimo']:.2f}")
                formatted.append(f"  ‚Ä¢ M√°ximo: {values['maximo']:.2f}")
                formatted.append(f"  ‚Ä¢ Desviaci√≥n Est√°ndar: {values['desviacion_std']:.2f}")
                formatted.append(f"  ‚Ä¢ Total de registros v√°lidos: {values['count']:,}")
                formatted.append("")  # L√≠nea en blanco
        
        # Estad√≠sticas categ√≥ricas
        if 'categorical' in stats and stats['categorical']:
            formatted.append("\nüìã COLUMNAS CATEG√ìRICAS:\n")
            
            for col, values in stats['categorical'].items():
                formatted.append(f"**{col}** ({values['valores_unicos']} valores √∫nicos)")
                formatted.append(f"  Top 5 m√°s frecuentes:")
                
                for i, item in enumerate(values['top_5'], 1):
                    formatted.append(f"  {i}. {item['value']}: {item['frequency']:,} registros ({item['percentage']:.1f}%)")
                
                formatted.append("")  # L√≠nea en blanco
        
        # Resumen
        total_numeric = len(stats.get('numeric', {}))
        total_categorical = len(stats.get('categorical', {}))
        
        formatted.append(f"\nüìà RESUMEN:")
        formatted.append(f"  ‚Ä¢ {total_numeric} columnas num√©ricas analizadas")
        formatted.append(f"  ‚Ä¢ {total_categorical} columnas categ√≥ricas analizadas")
        formatted.append(f"  ‚Ä¢ Estad√≠sticas calculadas en tiempo real con SQL")
        
        return "\n".join(formatted)
    
    async def _generate_response(self, prompt: str) -> str:
        """
        Genera respuesta con Gemini 2.0 Flash
        
        Configuraci√≥n optimizada para respuestas r√°pidas y precisas
        """
        try:
            loop = asyncio.get_event_loop()
            
            # Configuraci√≥n de generaci√≥n
            generation_config = {
                'temperature': 0.7,  # Balance entre creatividad y precisi√≥n
                'top_p': 0.95,
                'top_k': 40,
                'max_output_tokens': 2048,
            }
            
            response = await loop.run_in_executor(
                None,
                lambda: model.generate_content(
                    prompt,
                    generation_config=generation_config
                )
            )
            
            return response.text
            
        except Exception as e:
            print(f"‚ùå Error en generaci√≥n con Gemini: {e}")
            return f"Error generando respuesta con el modelo AI: {str(e)}"
    
    def get_conversation_summary(self, session_id: str) -> Dict[str, Any]:
        """Obtiene resumen de la conversaci√≥n"""
        history = self.conversation_manager.get_conversation_history(session_id)
        intent_pattern = self.conversation_manager.get_intent_pattern(session_id)
        active_file = self.conversation_manager.get_active_file(session_id)
        
        return {
            'total_messages': len(history),
            'intent_pattern': intent_pattern,
            'active_file': active_file,
            'recent_intents': intent_pattern[-5:] if intent_pattern else []
        }
    
    def reset_conversation(self, session_id: str) -> Dict[str, str]:
        """Reinicia una conversaci√≥n"""
        self.conversation_manager.clear_conversation(session_id)
        return {
            "success": True,
            "message": f"Conversaci√≥n {session_id} reiniciada"
        }


# Instancia global
ai_controller = AIController()
